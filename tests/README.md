# Test Suite Documentation

## ğŸ“Š Overview

This test suite provides comprehensive unit and integration testing for the Vision-based PDF AI Assistant. The suite includes **56 test cases** covering data models, API clients, vision analysis, and edge cases.

**Test Results**: âœ… **56/56 passing** (100% success rate)  
**Execution Time**: ~2.8 seconds  
**Test Coverage**: 40% overall, 97% for document models, 92% for vision analyzer

---

## ğŸ—‚ï¸ Test Structure

```
tests/
â”œâ”€â”€ __init__.py                    # Test package initialization
â”œâ”€â”€ conftest.py                    # Shared fixtures and configuration
â”œâ”€â”€ test_document_models.py        # Data model tests (25 tests)
â”œâ”€â”€ test_openai_client.py          # API client tests (15 tests)
â””â”€â”€ test_vision_analyzer.py        # Vision analysis tests (16 tests)
```

---

## ğŸ“‹ Test Cases Summary

### 1. Document Models Tests (`test_document_models.py`)

**Total**: 25 test cases  
**Coverage**: 97% of `app/processors/document.py`

#### TestTableInfo (3 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_create_table_info` | Verify TableInfo object creation | âœ… |
| `test_table_info_to_dict` | Test serialization to dictionary | âœ… |
| `test_table_info_from_dict` | Test deserialization from dictionary | âœ… |

**What it tests**: Table metadata structure with ID, title, and summary fields.

#### TestChartInfo (3 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_create_chart_info` | Verify ChartInfo object creation | âœ… |
| `test_chart_info_to_dict` | Test serialization with chart_type field | âœ… |
| `test_chart_info_from_dict` | Test deserialization including chart_type | âœ… |

**What it tests**: Chart metadata with ID, title, type (line/bar/pie), and summary.

#### TestPage (5 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_create_page` | Create page with basic properties | âœ… |
| `test_page_with_tables` | Page containing table metadata | âœ… |
| `test_page_with_charts` | Page containing chart metadata | âœ… |
| `test_page_without_tables_or_charts` | Empty page with no content | âœ… |
| `test_page_serialization` | Full serialization/deserialization cycle | âœ… |

**What it tests**: Page model with image path, dimensions, summary, tables, and charts. Helper methods: `has_tables()`, `has_charts()`, `get_table_count()`, `get_chart_count()`.

#### TestPartition (3 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_create_partition` | Create partition with ID and page range | âœ… |
| `test_partition_page_count` | Calculate pages in partition (21-35 = 15 pages) | âœ… |
| `test_partition_serialization` | Serialize page_range as list, deserialize as tuple | âœ… |

**What it tests**: Partition model for grouping pages in large documents (>20 pages).

#### TestDocument (4 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_create_small_document` | Document with 10 pages, no partitions | âœ… |
| `test_create_large_document` | Document with 50 pages, 3 partitions | âœ… |
| `test_document_serialization` | Full document serialization with status | âœ… |
| `test_document_with_partitions` | Verify page-to-partition assignment | âœ… |

**What it tests**: Document model with pages, partitions, status tracking. Methods: `is_large_document()`, `has_partitions()`.

#### TestPartitionDetails (2 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_partition_detail_with_tables_and_charts` | Partition with aggregated metadata | âœ… |
| `test_partition_details_serialization` | Full partition summary structure | âœ… |

**What it tests**: `partition_summary.json` file structure with aggregated tables/charts per partition.

#### TestEdgeCases (5 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_empty_page_list` | Document with 0 pages | âœ… |
| `test_page_with_no_dimensions` | Page without width/height | âœ… |
| `test_partition_single_page` | Partition with only 1 page | âœ… |
| `test_document_boundary_20_pages` | Exactly 20 pages â†’ no partitions | âœ… |
| `test_document_boundary_21_pages` | 21 pages â†’ creates partitions | âœ… |

**What it tests**: Boundary conditions and edge cases in document processing.

---

### 2. OpenAI Client Tests (`test_openai_client.py`)

**Total**: 15 test cases  
**Coverage**: 68% of `app/ai/openai.py`

#### TestOpenAIClient (7 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_init_with_api_key` | Initialize with explicit API key | âœ… |
| `test_init_from_env` | Initialize from environment variable | âœ… |
| `test_init_without_api_key` | Raise ValueError when no key provided | âœ… |
| `test_model_selection` | Select model based on document size (â‰¤20 vs >20) | âœ… |
| `test_chat_completion_basic` | Basic async chat completion call | âœ… |
| `test_vision_completion_single_image` | Vision API with 1 image | âœ… |
| `test_vision_completion_multiple_images` | Vision API with 3 images | âœ… |

**What it tests**: OpenAI client initialization, model selection, and basic API calls with mocked responses.

#### TestOpenAIClientErrorHandling (4 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_api_error_handling` | Handle generic API errors | âœ… |
| `test_empty_messages_list` | Handle empty message list | âœ… |
| `test_invalid_model_name` | Handle non-existent model error | âœ… |
| `test_timeout_handling` | Handle request timeout | âœ… |

**What it tests**: Error handling for various API failure scenarios.

#### TestOpenAIClientEdgeCases (4 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_very_long_api_key` | API key with 1000+ characters | âœ… |
| `test_special_characters_in_api_key` | API key with special chars (!@#$%) | âœ… |
| `test_empty_response_handling` | API returns empty string | âœ… |
| `test_very_large_response` | API returns 100KB+ response | âœ… |

**What it tests**: Unusual but valid inputs and responses.

---

### 3. Vision Analyzer Tests (`test_vision_analyzer.py`)

**Total**: 16 test cases  
**Coverage**: 92% of `app/ai/vision_analyzer.py`

#### TestVisionAnalyzer (8 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_init_with_storage_root` | Custom storage directory | âœ… |
| `test_init_from_env` | Storage from FLEX_RAG_DATA_LOCATION | âœ… |
| `test_encode_image` | Base64 encoding of JPEG images | âœ… |
| `test_analyze_single_page_success` | Successful page analysis with tables/charts | âœ… |
| `test_analyze_single_page_no_tables_charts` | Page with no tables or charts | âœ… |
| `test_analyze_single_page_json_with_code_fence` | Handle ```json``` wrapped responses | âœ… |
| `test_analyze_single_page_error_handling` | Graceful failure returns empty result | âœ… |
| `test_analyze_single_page_malformed_json` | Invalid JSON returns empty result | âœ… |

**What it tests**: Vision analyzer initialization, image encoding, and single page analysis with various response formats.

#### TestVisionAnalyzerPartitions (3 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_analyze_partition_batch_basic` | Analyze 5-page partition | âœ… |
| `test_analyze_partition_batch_large_partition` | Analyze 15-page partition (sampling to 10) | âœ… |
| `test_analyze_partition_batch_error_handling` | API failure returns empty summary | âœ… |

**What it tests**: Batch analysis of page partitions for large documents.

#### TestVisionAnalyzerDocumentAnalysis (2 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_analyze_small_document` | Full analysis of 10-page document | âœ… |
| `test_analyze_large_document` | Full analysis of 50-page document with partitions | âœ… |

**What it tests**: End-to-end document analysis workflow.

#### TestVisionAnalyzerEdgeCases (3 tests)
| Test | Purpose | Status |
|------|---------|--------|
| `test_analyze_page_image_not_found` | Missing image file returns empty result | âœ… |
| `test_analyze_partition_no_pages` | Empty partition list returns empty summary | âœ… |
| `test_save_metadata_io_error` | IO error during metadata save | âœ… |

**What it tests**: Error conditions and graceful degradation.

---

## ğŸ¯ Test Coverage by Component

| Component | Statements | Missing | Coverage | Critical Paths |
|-----------|-----------|---------|----------|----------------|
| `document.py` | 131 | 4 | **97%** âœ… | Serialization, validation |
| `vision_analyzer.py` | 146 | 11 | **92%** âœ… | Page analysis, partitioning |
| `openai.py` | 38 | 12 | **68%** âš ï¸ | API calls (mocked) |
| `page_selection_agent.py` | 282 | 282 | **0%** âŒ | Not yet tested |
| `pdf_to_image.py` | 125 | 125 | **0%** âŒ | Not yet tested |

**Overall Coverage**: 40% (434/722 statements covered)

### Coverage Goals
- âœ… **Achieved**: Document models (97%), Vision analyzer (92%)
- âš ï¸ **Partial**: OpenAI client (68% - API calls are mocked)
- âŒ **Missing**: Page selector, PDF processor (requires integration tests)

---

## ğŸ”§ Running Tests

### Quick Start
```bash
# Activate virtual environment
venv\Scripts\activate

# Run all tests
pytest

# Run with verbose output
pytest -v

# Run with coverage
pytest --cov=app --cov-report=html
```

### Run Specific Tests
```bash
# Run single test file
pytest tests/test_document_models.py

# Run specific test class
pytest tests/test_document_models.py::TestPage

# Run specific test
pytest tests/test_document_models.py::TestPage::test_create_page

# Run tests matching pattern
pytest -k "document"

# Run only async tests
pytest -m asyncio
```

### Useful Options
```bash
# Stop at first failure
pytest -x

# Show local variables on failure
pytest --showlocals

# Drop into debugger on failure
pytest --pdb

# Show print statements
pytest -s

# Run in parallel (faster)
pytest -n auto
```

---

## ğŸ§© Test Fixtures

Defined in `tests/conftest.py`:

### Document Fixtures
- **`sample_small_document()`**: 10-page document, no partitions
- **`sample_large_document()`**: 50-page document, 3 partitions (20+20+10)
- **`sample_page_with_tables()`**: Page with 2 tables
- **`sample_page_with_charts()`**: Page with 2 charts (line + pie)

### Mock Fixtures
- **`mock_openai_client()`**: Mocked OpenAI API client
  - `chat_completion()` â†’ AsyncMock
  - `vision_completion()` â†’ AsyncMock
  - Models: gpt-4-vision-preview

### Environment Fixtures
- **`mock_env_vars()`**: Set test environment variables
  - `OPENAI_API_KEY=test-api-key-12345`
  - `FLEX_RAG_DATA_LOCATION=/tmp/test_flex_rag`

### Utility Fixtures
- **`temp_test_dir(tmp_path)`**: Temporary directory with structure:
  ```
  temp_test_dir/
  â”œâ”€â”€ documents/
  â”œâ”€â”€ cache/
  â””â”€â”€ uploads/
  ```

---

## ğŸ¨ Test Categories

### Unit Tests (48 tests)
Tests individual components in isolation with mocked dependencies.
- Data models: 25 tests
- API client: 15 tests
- Vision analyzer: 8 tests

### Integration Tests (8 tests)
Tests interactions between components.
- Vision analyzer with document models: 5 tests
- Full document analysis workflow: 3 tests

### Async Tests (16 tests)
Tests asynchronous operations marked with `@pytest.mark.asyncio`.
- API calls: 7 tests
- Vision analysis: 8 tests
- Error handling: 1 test

---

## ğŸš¨ Edge Cases Covered

### Data Validation
- âœ… Empty documents (0 pages)
- âœ… Single-page documents
- âœ… Boundary conditions (20 vs 21 pages)
- âœ… Pages without dimensions
- âœ… Single-page partitions

### API Responses
- âœ… Empty responses
- âœ… Malformed JSON
- âœ… JSON with code fence markers (```json```)
- âœ… Very large responses (100KB+)
- âœ… API timeouts
- âœ… Invalid model names

### File Operations
- âœ… Missing image files
- âœ… IO errors during save
- âœ… Very long API keys (1000+ chars)
- âœ… Special characters in keys

### Error Handling
- âœ… API errors return defaults, don't crash
- âœ… File not found returns empty result
- âœ… Invalid inputs raise ValueError
- âœ… All errors are logged

---

## ğŸ“ˆ Test Metrics

### Execution Performance
- **Total Tests**: 56
- **Passed**: 56 (100%)
- **Failed**: 0
- **Skipped**: 0
- **Execution Time**: 2.8 seconds
- **Average per test**: 50ms

### Test Distribution
```
Document Models:    25 tests (44.6%)
OpenAI Client:      15 tests (26.8%)
Vision Analyzer:    16 tests (28.6%)
```

### Test Types
```
Synchronous:        40 tests (71.4%)
Asynchronous:       16 tests (28.6%)
```

---

## ğŸ” What's NOT Tested Yet

### Missing Test Coverage
1. **Page Selection Agent** (`page_selection_agent.py`)
   - Partition selection logic
   - Page selection within partitions
   - Q&A answering workflow
   - Coverage: 0%

2. **PDF to Image Processor** (`pdf_to_image.py`)
   - PDF parsing with PyMuPDF
   - Image conversion and resizing
   - Partition creation
   - Coverage: 0%

3. **Main Application** (`main.py`)
   - Streamlit UI interactions
   - File upload handling
   - User workflows

### Recommended Next Tests
1. Integration tests for page selection agent
2. Integration tests for PDF processing
3. End-to-end tests for full workflows
4. Performance tests for large documents (100+ pages)
5. UI tests with Streamlit testing framework

---

## ğŸ› ï¸ Adding New Tests

### 1. Create Test File
```bash
# In tests/ directory
touch test_new_feature.py
```

### 2. Write Test Structure
```python
import pytest
from unittest.mock import Mock, AsyncMock

class TestNewFeature:
    """Test new feature functionality"""
    
    def test_basic_case(self):
        """Test basic functionality"""
        result = function_to_test(input_data)
        assert result == expected_output
    
    @pytest.mark.asyncio
    async def test_async_case(self):
        """Test async functionality"""
        result = await async_function()
        assert result is not None
```

### 3. Run Tests
```bash
pytest tests/test_new_feature.py -v
```

---

## ğŸ“Š Coverage Report

### Generate Coverage Report
```bash
# Terminal report
pytest --cov=app --cov-report=term-missing

# HTML report (recommended)
pytest --cov=app --cov-report=html

# Open HTML report
htmlcov/index.html
```

### Coverage Output Example
```
Name                         Stmts   Miss  Cover   Missing
----------------------------------------------------------
app/processors/document.py     131      4    97%   151, 160, 178, 188
app/ai/vision_analyzer.py      146     11    92%   95, 140-141, 182, ...
app/ai/openai.py                38     12    68%   72, 101-133
----------------------------------------------------------
TOTAL                          722    434    40%
```

---

## ğŸ“ Best Practices Followed

### âœ… Do's (Implemented)
- âœ… Descriptive test names: `test_document_boundary_20_pages`
- âœ… Test both success and failure paths
- âœ… Mock external dependencies (OpenAI API)
- âœ… Use fixtures for reusable test data
- âœ… Test edge cases and boundaries
- âœ… Clean up resources (temp files)
- âœ… Async/await for async functions
- âœ… Type hints in test code
- âœ… Comprehensive docstrings

### âŒ Don'ts (Avoided)
- âŒ No hard-coded test data in test files
- âŒ No tests depending on external services
- âŒ No tests depending on other tests
- âŒ No untested edge cases
- âŒ No unclear test names like `test1`, `test2`

---

## ğŸ”— Related Documentation

- **Full Testing Guide**: `TESTING.md` - Comprehensive testing strategies
- **Quick Reference**: `TESTING_QUICK_REFERENCE.md` - Common commands
- **Error Handling**: `ERROR_HANDLING_REVIEW.md` - Robustness review
- **Project README**: `README.md` - Main project documentation

---

## ğŸ“ Support

### Test Failures?
1. Check error message in terminal
2. Run with `pytest --showlocals` for variable inspection
3. Use `pytest --pdb` to debug interactively
4. Check `htmlcov/index.html` for coverage gaps

### Adding New Tests?
1. Follow existing test patterns
2. Use fixtures from `conftest.py`
3. Mock external dependencies
4. Test edge cases
5. Run `pytest --cov` to verify coverage

---

**Last Updated**: November 8, 2025  
**Test Suite Version**: 1.0  
**Total Test Cases**: 56  
**Success Rate**: 100% âœ…  
**Coverage**: 40% overall, 97% critical paths
