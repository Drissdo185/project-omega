"""
Vision-based document analysis service
Analyzes document images and generates summaries with labels
"""
import json
from typing import List, Optional
from pathlib import Path
from loguru import logger
from app.models.document import Document, Page
from app.providers.base import BaseProvider


class VisionAnalysisService:
    """Service for analyzing documents using vision models"""

    def __init__(self, provider: BaseProvider, storage_root: str = None):
        """
        Initialize the vision analysis service
        
        Args:
            provider: LLM provider with vision capabilities
            storage_root: Root directory for storing analysis results
        """
        self.provider = provider

        if storage_root is None:
            import os
            storage_root = os.environ.get("FLEX_RAG_DATA_LOCATION", "./app/flex_rag_data_location")

        self.storage_root = Path(storage_root)
        self.documents_dir = self.storage_root / "documents"
        self.documents_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"VisionAnalysisService initialized with storage: {self.documents_dir}")

    async def analyze_page(
        self,
        page: Page,
        context: str = ""
    ) -> str:
        """
        Analyze a single page using vision model and return page summary

        Args:
            page: Page object with image path
            context: Optional context about the document

        Returns:
            str: Page summary generated by LLM
        """
        try:
            logger.info(f"Analyzing page {page.page_number}: {page.image_path}")

            # Build the prompt
            prompt = self._build_analysis_prompt(context)

            # Prepare multimodal message
            messages = [
                {
                    "role": "system",
                    "content": "You are an expert document analyst. Analyze images and provide structured information."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_path", "image_path": page.image_path, "detail": "high"},
                    ],
                },
            ]

            # Get analysis from LLM
            response = await self.provider.process_multimodal_messages(
                messages=messages,
                max_tokens=1000,
            )

            # Parse the response to get page summary
            page_summary = self._parse_page_summary(response)

            # Get cost
            cost = self.provider.get_last_cost() or 0.0
            logger.info(f"Page {page.page_number} analyzed (cost: ${cost:.4f})")

            return page_summary

        except Exception as e:
            logger.error(f"Failed to analyze page {page.page_number}: {e}")
            return "Analysis failed"

    async def analyze_document(
        self,
        document: Document,
        max_pages: Optional[int] = None
    ) -> Document:
        """
        Analyze entire document with all pages and save metadata

        Args:
            document: Document object with pages
            max_pages: Optional limit on number of pages to analyze

        Returns:
            Document object with updated summaries
        """
        try:
            logger.info(f"Starting analysis of document: {document.name} ({document.page_count} pages)")

            # Analyze each page
            pages_to_analyze = document.pages[:max_pages] if max_pages else document.pages

            for page in pages_to_analyze:
                # Provide context about the document
                context = f"This is page {page.page_number} of {document.page_count} from '{document.name}'"

                # Get page summary from LLM
                page.summary = await self.analyze_page(page, context)

            # Create document-level summary
            document.summary = f"Document with {len(pages_to_analyze)} pages analyzed"

            # Save complete metadata to JSON
            self._save_document_metadata(document)

            logger.info(f"Document analysis complete for: {document.name}")
            return document

        except Exception as e:
            logger.error(f"Failed to analyze document {document.id}: {e}")
            raise

    def _build_analysis_prompt(self, context: str = "") -> str:
        """Build prompt for page analysis"""
        return f"""Analyze this document page carefully and produce a comprehensive yet concise summary that captures **all key ideas, definitions, classifications, examples, and relationships** presented on this page.

Your summary should:
- Include all major concepts, terms, and categories mentioned.
- Preserve the logical flow and hierarchy of information (e.g., headings, subheadings, lists).
- Mention any diagrams, figures, or examples if they convey important ideas.
- Avoid unnecessary repetition or filler text.

IMPORTANT:
Return ONLY a plain text summary â€” do NOT include JSON, markdown, or formatting.
Keep it factual, clear, and written in complete sentences.
{context}
"""

    def _parse_page_summary(self, response: str) -> str:
        """Extract page summary from LLM response"""
        try:
            summary = response.strip()
            summary = summary.replace("**", "").replace("*", "")
            if len(summary) > 500:
                summary = summary[:497] + "..."
            return summary if summary else "No summary available"
        except Exception as e:
            logger.error(f"Failed to parse page summary: {e}")
            return "Summary parsing failed"

    def _save_document_metadata(self, document: Document):
        """Save document metadata to metadata.json file"""
        try:
            doc_dir = self.documents_dir / document.id
            doc_dir.mkdir(parents=True, exist_ok=True)
            output_path = doc_dir / "metadata.json"

            metadata = {
                "id": document.id,
                "name": document.name,
                "page_count": document.page_count,
                "status": document.status.value,
                "summary": document.summary,
                "created_at": document.created_at.isoformat(),
                "updated_at": document.updated_at.isoformat(),
                "pages": [p.to_dict() for p in document.pages],
            }

            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)

            logger.info(f"Saved complete metadata to: {output_path}")

            self._update_index(document)

        except Exception as e:
            logger.error(f"Failed to save metadata: {e}")

    def _update_index(self, document: Document):
        """Update or create the global document index"""
        try:
            index_path = self.documents_dir / "index.json"
            if index_path.exists():
                with open(index_path, "r", encoding="utf-8") as f:
                    index = json.load(f)
            else:
                index = []

            index_entry = {
                "id": document.id,
                "name": document.name,
                "page_count": document.page_count,
                "status": document.status.value,
                "created_at": document.created_at.isoformat(),
                "updated_at": document.updated_at.isoformat(),
            }

            index = [e for e in index if e.get("id") != document.id]
            index.append(index_entry)

            with open(index_path, "w", encoding="utf-8") as f:
                json.dump(index, f, indent=2, ensure_ascii=False)

            logger.debug(f"Updated index at {index_path}")

        except Exception as e:
            logger.error(f"Failed to update index: {e}")

    def load_document(self, document_id: str) -> Optional[Document]:
        """Load previously saved document from metadata.json"""
        try:
            metadata_path = self.documents_dir / document_id / "metadata.json"
            if not metadata_path.exists():
                return None

            with open(metadata_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            return Document.from_dict(data)
        except Exception as e:
            logger.error(f"Failed to load document for {document_id}: {e}")
            return None

    def get_all_documents(self) -> List[Document]:
        """Load all saved documents from metadata.json files"""
        documents = []
        for doc_dir in self.documents_dir.iterdir():
            if not doc_dir.is_dir() or doc_dir.name == "index.json":
                continue

            metadata_file = doc_dir / "metadata.json"
            if not metadata_file.exists():
                continue

            try:
                with open(metadata_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                document = Document.from_dict(data)
                documents.append(document)
            except Exception as e:
                logger.error(f"Failed to load {metadata_file}: {e}")

        return documents
